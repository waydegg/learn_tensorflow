{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Linear Model with Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensorflow's ```tf.estimator``` API, we will use census data of people's age, education, maritial status, and occupation to predict wether they make more than $50,000 a year. We will train a *logistic regression* model that outputs a number between 0 and 1 (the probability they make or dont make more than 50k a year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.feature_column as fc\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download official implmentation (TF's \"wide and deep model\" from the TF model repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'models' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -q requests\n",
    "! git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the root directory of the repository to the Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "sys.path.append(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.wide_deep import census_dataset\n",
    "from official.wide_deep import census_main\n",
    "\n",
    "census_dataset.download(\"/tmp/census_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the repo's included program to expirament with this type of model, add ```tensorflow/models``` to ```PYTHONPATH``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PYTHONPATH\" in os.environ:\n",
    "    os.environ[\"PYTHONPATH\"] += os.pathsep + models_path\n",
    "else:\n",
    "    os.environ[\"PYTHONPATH\"] = models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DNN on census income dataset.\r\n",
      "flags:\r\n",
      "\r\n",
      "/home/waydegg/development/projects/loftyai/_learning/tensorflow/models/official/wide_deep/census_main.py:\r\n",
      "  -bs,--batch_size:\r\n",
      "    Batch size for training and evaluation. When using multiple gpus, this is\r\n",
      "    the\r\n",
      "    global batch size for all devices. For example, if the batch size is 32 and\r\n",
      "    there are 4 GPUs, each GPU will get 8 examples on each step.\r\n",
      "    (default: '40')\r\n",
      "    (an integer)\r\n",
      "  --[no]clean:\r\n",
      "    If set, model_dir will be removed if it exists.\r\n",
      "    (default: 'false')\r\n",
      "  -dd,--data_dir:\r\n",
      "    The location of the input data.\r\n",
      "    (default: '/tmp/census_data')\r\n",
      "  --[no]download_if_missing:\r\n",
      "    Download data to data_dir if it is not already present.\r\n",
      "    (default: 'true')\r\n",
      "  -ebe,--epochs_between_evals:\r\n",
      "    The number of training epochs to run between evaluations.\r\n",
      "    (default: '2')\r\n",
      "    (an integer)\r\n",
      "  -ed,--export_dir:\r\n",
      "    If set, a SavedModel serialization of the model will be exported to this\r\n",
      "    directory at the end of training. See the README for more details and\r\n",
      "    relevant\r\n",
      "    links.\r\n",
      "  -hk,--hooks:\r\n",
      "    A list of (case insensitive) strings to specify the names of training hooks.\r\n",
      "    ﻿  Hook:\r\n",
      "    ﻿    loggingtensorhook\r\n",
      "    ﻿    profilerhook\r\n",
      "    ﻿    examplespersecondhook\r\n",
      "    ﻿    loggingmetrichook\r\n",
      "    ﻿  Example: `--hooks ProfilerHook,ExamplesPerSecondHook`\r\n",
      "    See official.utils.logs.hooks_helper for details.\r\n",
      "    (default: 'LoggingTensorHook')\r\n",
      "    (a comma separated list)\r\n",
      "  -md,--model_dir:\r\n",
      "    The location of the model checkpoint files.\r\n",
      "    (default: '/tmp/census_model')\r\n",
      "  -mt,--model_type: <wide|deep|wide_deep>: Select model topology.\r\n",
      "    (default: 'wide_deep')\r\n",
      "  -te,--train_epochs:\r\n",
      "    The number of epochs used to train.\r\n",
      "    (default: '40')\r\n",
      "    (an integer)\r\n",
      "\r\n",
      "Try --helpfull to get a list of all flags.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m official.wide_deep.census_main --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0619 20:52:58.103579 140369188927296 estimator.py:201] Using config: {'_model_dir': '/tmp/census_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "  value: 0\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa9d6140e80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W0619 20:52:58.104118 140369188927296 tf_logging.py:161] 'cpuinfo' not imported. CPU info will not be logged.\n",
      "2019-06-19 20:52:58.104257: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-06-19 20:52:58.141702: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3799545000 Hz\n",
      "2019-06-19 20:52:58.142865: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559c0371a0b0 executing computations on platform Host. Devices:\n",
      "2019-06-19 20:52:58.142889: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-19 20:52:58.364808: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559c033a8690 executing computations on platform CUDA. Devices:\n",
      "2019-06-19 20:52:58.364853: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2019-06-19 20:52:58.364864: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2019-06-19 20:52:58.365618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\n",
      "pciBusID: 0000:0a:00.0\n",
      "totalMemory: 10.92GiB freeMemory: 10.76GiB\n",
      "2019-06-19 20:52:58.365792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705\n",
      "pciBusID: 0000:42:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 3.02GiB\n",
      "2019-06-19 20:52:58.367389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1\n",
      "2019-06-19 20:52:58.369175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-19 20:52:58.369194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 \n",
      "2019-06-19 20:52:58.369202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y \n",
      "2019-06-19 20:52:58.369210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N \n",
      "2019-06-19 20:52:58.369700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10464 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)\n",
      "2019-06-19 20:52:58.370052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 2797 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:42:00.0, compute capability: 6.1)\n",
      "W0619 20:52:58.371458 140369188927296 tf_logging.py:161] 'psutil' not imported. Memory info will not be logged.\n",
      "I0619 20:52:58.403419 140369188927296 logger.py:152] Benchmark run: {'model_name': 'wide_deep', 'dataset': {'name': 'Census Income'}, 'machine_config': {'gpu_info': {'count': 2, 'model': 'GeForce GTX 1080 Ti'}}, 'test_id': None, 'run_date': '2019-06-20T03:52:58.103842Z', 'tensorflow_version': {'version': '1.13.1', 'git_hash': \"b'unknown'\"}, 'tensorflow_environment_variables': [], 'run_parameters': [{'name': 'batch_size', 'long_value': 40}, {'name': 'model_type', 'string_value': 'wide'}, {'name': 'train_epochs', 'long_value': 2}]}\n",
      "W0619 20:52:58.407890 140369188927296 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "I0619 20:52:58.419743 140369188927296 census_dataset.py:167] Parsing /tmp/census_data/adult.data\n",
      "I0619 20:52:58.443108 140369188927296 estimator.py:1111] Calling model_fn.\n",
      "W0619 20:52:58.472734 140369188927296 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "W0619 20:52:58.483494 140369188927296 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2898: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "I0619 20:52:59.182237 140369188927296 estimator.py:1113] Done calling model_fn.\n",
      "I0619 20:52:59.182519 140369188927296 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n",
      "I0619 20:52:59.442869 140369188927296 monitored_session.py:222] Graph was finalized.\n",
      "2019-06-19 20:52:59.443289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-19 20:52:59.443315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      \n",
      "W0619 20:52:59.444902 140369188927296 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0619 20:52:59.445827 140369188927296 saver.py:1270] Restoring parameters from /tmp/census_model/model.ckpt-1629\n",
      "W0619 20:52:59.510654 140369188927296 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "I0619 20:52:59.542915 140369188927296 session_manager.py:491] Running local_init_op.\n",
      "I0619 20:52:59.561424 140369188927296 session_manager.py:493] Done running local_init_op.\n",
      "I0619 20:53:00.114762 140369188927296 basic_session_run_hooks.py:594] Saving checkpoints for 1629 into /tmp/census_model/model.ckpt.\n",
      "I0619 20:53:00.586828 140369188927296 basic_session_run_hooks.py:249] average_loss = 0.23651047, loss = 9.460419\n",
      "I0619 20:53:00.587327 140369188927296 basic_session_run_hooks.py:249] loss = 9.460419, step = 1630\n",
      "I0619 20:53:01.090332 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 198.571\n",
      "I0619 20:53:01.091033 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.2946382, loss = 11.785527 (0.504 sec)\n",
      "I0619 20:53:01.091235 140369188927296 basic_session_run_hooks.py:247] loss = 11.785527, step = 1730 (0.504 sec)\n",
      "I0619 20:53:01.376858 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 348.761\n",
      "I0619 20:53:01.377476 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.36518535, loss = 14.607414 (0.286 sec)\n",
      "I0619 20:53:01.377674 140369188927296 basic_session_run_hooks.py:247] loss = 14.607414, step = 1830 (0.286 sec)\n",
      "I0619 20:53:01.652515 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 362.78\n",
      "I0619 20:53:01.653173 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.262527, loss = 10.50108 (0.276 sec)\n",
      "I0619 20:53:01.653367 140369188927296 basic_session_run_hooks.py:247] loss = 10.50108, step = 1930 (0.276 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0619 20:53:01.924414 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 367.778\n",
      "I0619 20:53:01.925095 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.29963666, loss = 11.985467 (0.272 sec)\n",
      "I0619 20:53:01.925290 140369188927296 basic_session_run_hooks.py:247] loss = 11.985467, step = 2030 (0.272 sec)\n",
      "I0619 20:53:02.197356 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 366.386\n",
      "I0619 20:53:02.198033 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.25066406, loss = 10.026562 (0.273 sec)\n",
      "I0619 20:53:02.198242 140369188927296 basic_session_run_hooks.py:247] loss = 10.026562, step = 2130 (0.273 sec)\n",
      "I0619 20:53:02.463029 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 376.382\n",
      "I0619 20:53:02.463628 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.33679628, loss = 13.471851 (0.266 sec)\n",
      "I0619 20:53:02.463823 140369188927296 basic_session_run_hooks.py:247] loss = 13.471851, step = 2230 (0.266 sec)\n",
      "I0619 20:53:02.736534 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 365.64\n",
      "I0619 20:53:02.737175 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.28318626, loss = 11.32745 (0.274 sec)\n",
      "I0619 20:53:02.737375 140369188927296 basic_session_run_hooks.py:247] loss = 11.32745, step = 2330 (0.274 sec)\n",
      "I0619 20:53:03.007627 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 368.863\n",
      "I0619 20:53:03.008308 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.31145123, loss = 12.458049 (0.271 sec)\n",
      "I0619 20:53:03.008536 140369188927296 basic_session_run_hooks.py:247] loss = 12.458049, step = 2430 (0.271 sec)\n",
      "I0619 20:53:03.320257 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 319.87\n",
      "I0619 20:53:03.320852 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.38718098, loss = 15.487239 (0.313 sec)\n",
      "I0619 20:53:03.321045 140369188927296 basic_session_run_hooks.py:247] loss = 15.487239, step = 2530 (0.313 sec)\n",
      "I0619 20:53:03.589421 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 371.524\n",
      "I0619 20:53:03.589954 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.40379563, loss = 16.151825 (0.269 sec)\n",
      "I0619 20:53:03.590099 140369188927296 basic_session_run_hooks.py:247] loss = 16.151825, step = 2630 (0.269 sec)\n",
      "I0619 20:53:04.060431 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 212.322\n",
      "I0619 20:53:04.060962 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.285041, loss = 11.40164 (0.471 sec)\n",
      "I0619 20:53:04.061117 140369188927296 basic_session_run_hooks.py:247] loss = 11.40164, step = 2730 (0.471 sec)\n",
      "I0619 20:53:04.335799 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 363.243\n",
      "I0619 20:53:04.336576 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.35324457, loss = 14.129783 (0.276 sec)\n",
      "I0619 20:53:04.336787 140369188927296 basic_session_run_hooks.py:247] loss = 14.129783, step = 2830 (0.276 sec)\n",
      "I0619 20:53:04.605082 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 371.268\n",
      "I0619 20:53:04.605740 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.38676587, loss = 15.470634 (0.269 sec)\n",
      "I0619 20:53:04.605937 140369188927296 basic_session_run_hooks.py:247] loss = 15.470634, step = 2930 (0.269 sec)\n",
      "I0619 20:53:04.869664 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 377.909\n",
      "I0619 20:53:04.870272 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.5226445, loss = 20.90578 (0.265 sec)\n",
      "I0619 20:53:04.870469 140369188927296 basic_session_run_hooks.py:247] loss = 20.90578, step = 3030 (0.265 sec)\n",
      "I0619 20:53:05.139223 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 370.978\n",
      "I0619 20:53:05.139878 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.4111394, loss = 16.445576 (0.270 sec)\n",
      "I0619 20:53:05.140077 140369188927296 basic_session_run_hooks.py:247] loss = 16.445576, step = 3130 (0.270 sec)\n",
      "I0619 20:53:05.410792 140369188927296 basic_session_run_hooks.py:680] global_step/sec: 368.233\n",
      "I0619 20:53:05.411427 140369188927296 basic_session_run_hooks.py:247] average_loss = 0.28434816, loss = 11.373926 (0.272 sec)\n",
      "I0619 20:53:05.411625 140369188927296 basic_session_run_hooks.py:247] loss = 11.373926, step = 3230 (0.272 sec)\n",
      "I0619 20:53:05.492210 140369188927296 basic_session_run_hooks.py:594] Saving checkpoints for 3258 into /tmp/census_model/model.ckpt.\n",
      "I0619 20:53:05.696552 140369188927296 estimator.py:359] Loss for final step: 0.5501342.\n",
      "I0619 20:53:06.475192 140369188927296 census_dataset.py:167] Parsing /tmp/census_data/adult.test\n",
      "I0619 20:53:06.492125 140369188927296 estimator.py:1111] Calling model_fn.\n",
      "W0619 20:53:07.125870 140369188927296 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0619 20:53:07.359873 140369188927296 metrics_impl.py:783] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0619 20:53:07.372199 140369188927296 metrics_impl.py:783] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "I0619 20:53:07.384523 140369188927296 estimator.py:1113] Done calling model_fn.\n",
      "I0619 20:53:07.396209 140369188927296 evaluation.py:257] Starting evaluation at 2019-06-20T03:53:07Z\n",
      "I0619 20:53:07.481484 140369188927296 monitored_session.py:222] Graph was finalized.\n",
      "2019-06-19 20:53:07.482028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-19 20:53:07.482057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      \n",
      "I0619 20:53:07.482972 140369188927296 saver.py:1270] Restoring parameters from /tmp/census_model/model.ckpt-3258\n",
      "I0619 20:53:07.553126 140369188927296 session_manager.py:491] Running local_init_op.\n",
      "I0619 20:53:07.584714 140369188927296 session_manager.py:493] Done running local_init_op.\n",
      "I0619 20:53:08.713691 140369188927296 evaluation.py:277] Finished evaluation at 2019-06-20-03:53:08\n",
      "I0619 20:53:08.713964 140369188927296 estimator.py:1979] Saving dict for global step 3258: accuracy = 0.83514524, accuracy_baseline = 0.76377374, auc = 0.88327074, auc_precision_recall = 0.69362193, average_loss = 0.35211068, global_step = 3258, label/mean = 0.23622628, loss = 14.050769, precision = 0.6968157, prediction/mean = 0.23133725, recall = 0.5348414\n",
      "I0619 20:53:08.890605 140369188927296 estimator.py:2039] Saving 'checkpoint_path' summary for global step 3258: /tmp/census_model/model.ckpt-3258\n",
      "I0619 20:53:08.891065 140369188927296 wide_deep_run_loop.py:116] Results at epoch 2 / 2\n",
      "I0619 20:53:08.891115 140369188927296 wide_deep_run_loop.py:117] ------------------------------------------------------------\n",
      "I0619 20:53:08.891163 140369188927296 wide_deep_run_loop.py:120] accuracy: 0.83514524\n",
      "I0619 20:53:08.891201 140369188927296 wide_deep_run_loop.py:120] accuracy_baseline: 0.76377374\n",
      "I0619 20:53:08.891236 140369188927296 wide_deep_run_loop.py:120] auc: 0.88327074\n",
      "I0619 20:53:08.891271 140369188927296 wide_deep_run_loop.py:120] auc_precision_recall: 0.69362193\n",
      "I0619 20:53:08.891306 140369188927296 wide_deep_run_loop.py:120] average_loss: 0.35211068\n",
      "I0619 20:53:08.891345 140369188927296 wide_deep_run_loop.py:120] global_step: 3258\n",
      "I0619 20:53:08.891378 140369188927296 wide_deep_run_loop.py:120] label/mean: 0.23622628\n",
      "I0619 20:53:08.891412 140369188927296 wide_deep_run_loop.py:120] loss: 14.050769\n",
      "I0619 20:53:08.891444 140369188927296 wide_deep_run_loop.py:120] precision: 0.6968157\n",
      "I0619 20:53:08.891477 140369188927296 wide_deep_run_loop.py:120] prediction/mean: 0.23133725\n",
      "I0619 20:53:08.891509 140369188927296 wide_deep_run_loop.py:120] recall: 0.5348414\n",
      "I0619 20:53:08.891587 140369188927296 logger.py:147] Benchmark metric: {'name': 'accuracy', 'value': 0.8351452350616455, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891558Z', 'extras': []}\n",
      "I0619 20:53:08.891665 140369188927296 logger.py:147] Benchmark metric: {'name': 'accuracy_baseline', 'value': 0.7637737393379211, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891652Z', 'extras': []}\n",
      "I0619 20:53:08.891721 140369188927296 logger.py:147] Benchmark metric: {'name': 'auc', 'value': 0.8832707405090332, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891709Z', 'extras': []}\n",
      "I0619 20:53:08.891772 140369188927296 logger.py:147] Benchmark metric: {'name': 'auc_precision_recall', 'value': 0.6936219334602356, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891762Z', 'extras': []}\n",
      "I0619 20:53:08.891824 140369188927296 logger.py:147] Benchmark metric: {'name': 'average_loss', 'value': 0.35211068391799927, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891813Z', 'extras': []}\n",
      "I0619 20:53:08.891875 140369188927296 logger.py:147] Benchmark metric: {'name': 'label/mean', 'value': 0.23622627556324005, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891864Z', 'extras': []}\n",
      "I0619 20:53:08.891925 140369188927296 logger.py:147] Benchmark metric: {'name': 'loss', 'value': 14.050768852233887, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891915Z', 'extras': []}\n",
      "I0619 20:53:08.891975 140369188927296 logger.py:147] Benchmark metric: {'name': 'precision', 'value': 0.6968157291412354, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.891965Z', 'extras': []}\n",
      "I0619 20:53:08.892024 140369188927296 logger.py:147] Benchmark metric: {'name': 'prediction/mean', 'value': 0.23133724927902222, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.892014Z', 'extras': []}\n",
      "I0619 20:53:08.892073 140369188927296 logger.py:147] Benchmark metric: {'name': 'recall', 'value': 0.5348414182662964, 'unit': None, 'global_step': 3258, 'timestamp': '2019-06-20T03:53:08.892064Z', 'extras': []}\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "!python -m official.wide_deep.census_main --model_type=wide --train_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ```census_dataset.py``` which helps with cleaning up some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult.data  adult.test\r\n"
     ]
    }
   ],
   "source": [
    "! ls /tmp/census_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/tmp/census_data/adult.data\"\n",
    "test_file = \"/tmp/census_data/adult.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_file, header=None, names=census_dataset._CSV_COLUMNS)\n",
    "test_df = pd.read_csv(test_file, header=None, names=census_dataset._CSV_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital_status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \n",
       "0          2174             0              40  United-States          <=50K  \n",
       "1             0             0              13  United-States          <=50K  \n",
       "2             0             0              40  United-States          <=50K  \n",
       "3             0             0              40  United-States          <=50K  \n",
       "4             0             0              40           Cuba          <=50K  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ```input_function``` that prepares the data in a way that it can be inputted into the model. This function will return a ```tf.data.Dataset``` of batches of (```features-dict```, ```label```) pairs. It is not called until it is passed into the ```tf.estimator.Estimator``` methods such as ```train``` and ```evaluate```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_input_function(df, label_key, num_epochs, shuffle, batch_size):\n",
    "    label = df[label_key]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), label))\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    \n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With eager execution enabled, we can easily inspect the result of the input function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0619 20:53:10.015953 140536260691776 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature keys:  ['age', 'workclass', 'fnlwgt', 'education', 'education_num']\n",
      "\n",
      "A batch of Ages :  tf.Tensor([48 30 53 36 51 31 50 39 43 31], shape=(10,), dtype=int32)\n",
      "\n",
      "A batch of Labels:  tf.Tensor(\n",
      "[b'<=50K' b'<=50K' b'<=50K' b'>50K' b'>50K' b'>50K' b'<=50K' b'<=50K'\n",
      " b'>50K' b'<=50K'], shape=(10,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "ds = easy_input_function(train_df, label_key='income_bracket', num_epochs=5, shuffle=True, batch_size=10)\n",
    "\n",
    "for feature_batch, label_batch in ds.take(1):\n",
    "    print('Some feature keys: ', list(feature_batch.keys())[:5])\n",
    "    print()\n",
    "    print(\"A batch of Ages : \", feature_batch[\"age\"])\n",
    "    print()\n",
    "    print(\"A batch of Labels: \", label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the `input_fn` from the `census_dataset`. Larger datasets should be *streamed from disk*, and this function does just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
      "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
      "  assert tf.gfile.Exists(data_file), (\n",
      "      '%s not found. Please make sure you have run census_dataset.py and '\n",
      "      'set the --data_dir argument to the correct path.' % data_file)\n",
      "\n",
      "  def parse_csv(value):\n",
      "    tf.logging.info('Parsing {}'.format(data_file))\n",
      "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
      "    features = dict(zip(_CSV_COLUMNS, columns))\n",
      "    labels = features.pop('income_bracket')\n",
      "    classes = tf.equal(labels, '>50K')  # binary classification\n",
      "    return features, classes\n",
      "\n",
      "  # Extract lines from input files using the Dataset API.\n",
      "  dataset = tf.data.TextLineDataset(data_file)\n",
      "\n",
      "  if shuffle:\n",
      "    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
      "\n",
      "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
      "\n",
      "  # We call repeat after shuffling, rather than before, to prevent separate\n",
      "  # epochs from blending together.\n",
      "  dataset = dataset.repeat(num_epochs)\n",
      "  dataset = dataset.batch(batch_size)\n",
      "  return dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(census_dataset.input_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the input function to an object with an expected signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "train_inpf = functools.partial(census_dataset.input_fn, train_file, num_epochs=2, shuffle=True, batch_size=64)\n",
    "test_inpf = functools.partial(census_dataset.input_fn, test_file, num_epochs=1, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Feature Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric Columns (for Continuous features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = fc.numeric_column('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 20:53:10.141360 140536260691776 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 20:53:10.142695 140536260691776 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 20:53:10.143920 140536260691776 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 20:53:10.144951 140536260691776 deprecation.py:323] From /home/waydegg/anaconda3/envs/fastai-course-v3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[48.],\n",
       "       [30.],\n",
       "       [53.],\n",
       "       [36.],\n",
       "       [51.],\n",
       "       [31.],\n",
       "       [50.],\n",
       "       [39.],\n",
       "       [43.],\n",
       "       [31.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the feature column result\n",
    "fc.input_layer(feature_batch, [age]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a model using only the `age` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.74737424, 'accuracy_baseline': 0.76377374, 'auc': 0.67835975, 'auc_precision_recall': 0.31139234, 'average_loss': 0.52639776, 'label/mean': 0.23622628, 'loss': 33.60895, 'precision': 0.17675544, 'prediction/mean': 0.27435714, 'recall': 0.01898076, 'global_step': 1018}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=[age])\n",
    "classifier.train(train_inpf)\n",
    "result = classifier.evaluate(test_inpf)\n",
    "\n",
    "clear_output() # used for display in notebook\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a `NumericColumn` for each continuous feature that we want to use in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "\n",
       "       marital_status       occupation   relationship   race gender  \\\n",
       "0       Never-married     Adm-clerical  Not-in-family  White   Male   \n",
       "1  Married-civ-spouse  Exec-managerial        Husband  White   Male   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \n",
       "0          2174             0              40  United-States          <=50K  \n",
       "1             0             0              13  United-States          <=50K  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  48.,    0.,    0.,    9.,   40.],\n",
       "       [  30.,    0., 1719.,   10.,   25.],\n",
       "       [  53.,    0.,    0.,    4.,   40.],\n",
       "       [  36.,    0., 1902.,   13.,   50.],\n",
       "       [  51.,    0., 1564.,   16.,   70.],\n",
       "       [  31.,    0.,    0.,   13.,   40.],\n",
       "       [  50.,    0.,    0.,    7.,   40.],\n",
       "       [  39.,    0.,    0.,    9.,   40.],\n",
       "       [  43.,    0.,    0.,   10.,   50.],\n",
       "       [  31.,    0.,    0.,   12.,   40.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_num = tf.feature_column.numeric_column('education_num')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "my_numeric_columns = [age, education_num, capital_gain, capital_loss, hours_per_week]\n",
    "\n",
    "fc.input_layer(feature_batch, my_numeric_columns).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model on these features by changing the `feature_columns` argument to the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, 0.78140163\n",
      "accuracy_baseline, 0.76377374\n",
      "auc, 0.7933894\n",
      "auc_precision_recall, 0.56146234\n",
      "average_loss, 1.2828498\n",
      "global_step, 1018\n",
      "label/mean, 0.23622628\n",
      "loss, 81.90619\n",
      "precision, 0.56889105\n",
      "prediction/mean, 0.34047222\n",
      "recall, 0.30811232\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns)\n",
    "classifier.train(train_inpf)\n",
    "\n",
    "result = classifier.evaluate(test_inpf)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "for key,value in sorted(result.items()):\n",
    "    print('%s, %s' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship = fc.categorical_column_with_vocabulary_list(\n",
    "    'relationship',\n",
    "    ['Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `input_layer` function is designed for DNN (Dense Neural Network) models, it expects dense inputs. So, we must wrap the one-hot encoded categorical columns in a `tf.feature_column.indicator_column` to create the dense one-hot output (Linear `Estimators` can often skip this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4893, shape=(10, 7), dtype=float32, numpy=\n",
       "array([[48.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [30.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [53.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [36.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [51.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [31.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [50.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [39.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [43.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [31.,  0.,  0.,  0.,  0.,  1.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(feature_batch, [age, fc.indicator_column(relationship)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the set of possible values in advance (or there are just too many), we can use `categorical_column_with_hash_bucket`instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Craft-repair\n",
      "?\n",
      "Craft-repair\n",
      "Prof-specialty\n",
      "Prof-specialty\n",
      "Sales\n",
      "Other-service\n",
      "Craft-repair\n",
      "Exec-managerial\n",
      "Other-service\n"
     ]
    }
   ],
   "source": [
    "for item in feature_batch['occupation'].numpy():\n",
    "    print(item.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the hashed feature column, we now have an output shape of `(batch_size, hash_bucket_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occupation_result = fc.input_layer(feature_batch, [fc.indicator_column(occupation)])\n",
    "\n",
    "occupation_result.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([466,  65, 466, 979, 979, 631, 527, 466, 800, 527])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(occupation_result, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature columns for the other categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "\n",
    "my_categorical_columns = [relationship, occupation, education, marital_status, workclass]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear classifier with both numerical and categorical feature columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.83969045\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.885083\n",
      "auc_precision_recall: 0.71800274\n",
      "average_loss: 0.36432135\n",
      "global_step: 1018\n",
      "label/mean: 0.23622628\n",
      "loss: 23.260847\n",
      "precision: 0.70752186\n",
      "prediction/mean: 0.2146328\n",
      "recall: 0.5478419\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.LinearClassifier(feature_columns=my_numeric_columns+my_categorical_columns)\n",
    "classifier.train(train_inpf)\n",
    "result = classifier.evaluate(test_inpf)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "for key,value in sorted(result.items()):\n",
    "    print(\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derived Feature Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Make Continuous Features Categorical through Bucketization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When bucketing, the model sees each bucket as a one-hot feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [30.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [53.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [36.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [51.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [31.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [50.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [39.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [43.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [31.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(feature_batch, [age, age_buckets]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Learn complex relationships with crossed column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_x_occupation = tf.feature_column.crossed_column(\n",
    "    ['education', 'occupation'], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "?fc.crossed_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
